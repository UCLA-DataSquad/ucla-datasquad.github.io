---
layout: blogs
title: Using Tools to Automate the Extraction of Metadata and Text for Research
author: Aimee Xu
---

Over the Summer and Fall quarter, Lukas Hager, a fourth-year Math of Computation student on the UCLA Datasquad, explored methodologies that could aid researchers in analyzing UCLA Library archives. One common method, [Optical Character Recognition (OCR)](https://en.wikipedia.org/wiki/Optical_character_recognition), encompasses a broad group of technologies that uses pattern-matching algorithms to directly scan a document into a PDF with searchable text. UCLA archives contain texts from many years, mediums, and languages, whether it be from a political campaign image or an ancient transcription.  This work is especially helpful for making the content of scanned documents or images accessible for further analysis by UCLA researchers and collaborators.

## Step 1: Google Tesseract - Finding an OCR Engine
Hager began his research with [Tesseract software](https://en.wikipedia.org/wiki/Tesseract_(software)), an OCR engine developed in the 1980s by Hewlett-Packard and further improved and managed by Google. Tesseract is an open-source software that converts scanned documents or images into editable text. It supports over 100 languages, but quality can vary. Additional training may be necessary for some languages. The Tesseract community constantly develops the software for better quality and language support. Naturally, even with the benefits Tesseract provides, typos and errors arise. He recalls, “Certain languages [on Tesseract] may be more error-prone than others, specifically non-Latin-based scripts.” Working around this challenge was especially important for Hager’s project since many texts in UCLA collections contain non-Latin languages and languages like Arabic and Hebrew which are read by human eyes from right to left. Understanding and acknowledging issues with Teserract is essential since it undergirds many OCR engines. Being able to train and develop workarounds is an important first step.

## Step 2:  Named Entity Recognition and Named Entity Linking (NER/NEL) - Linking the Metadata
![Paris France NEL](https://en.wikipedia.org/wiki/File:Entity_Linking_-_Short_Example.png)

Using the text generated by OCR, [Named Entity Recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) can be applied to identify and categorize entities in the extracted text. The recognized named entities are then linked to relevant entries in a knowledge base. This linking process, [Named Entity Linking (NEL)](https://en.wikipedia.org/wiki/Entity_linking), involves associating the identified entities with unique identifiers or entries in a structured database. An entity is a word or group of words that can be linked to the same category. Common entity categories include Person, Organization, Place, and Location. Entity linking is particularly useful in producing a high-level overview of a large quantity of text. Researchers who used to have to sift through thousands of pages of text can now use NEL to understand the central theme and relevancy of a body of work.
        	
Hager explained, "NEL programs identify named figures within a text and link them to a Wikipedia and Wikidata database". He uses [ReFinED](https://www.amazon.science/publications/refined-an-efficient-zero-shot-capable-approach-to-end-to-end-entity-linking), an end-to-end NEL developed by Amazon Research, which links entity mentions in documents to their corresponding entities. People using his OCR software would not only have their documents scanned into searchable text but also have words that often appear in that text to be hyperlinked to their respective database entries. Lukas notes, "ReFinED is a great resource for hyperlinking mentions of a text, even with typos, but may not be very user-friendly.”

## Step 3: Analyzed Layout and Text Object (ALTO) - Displaying the Text
After comprehensive research around character recognition and entity linking, Hager sought to add a User Interface in his OCR pipeline to showcase the text from the steps above. This, in turn, would make metadata easily searchable to UCLA researchers and provide a workaround to the *blackbox nature* of Named Entity Linking. He came across ALTO and hOCR, two commonly used formats for OCR output. [Analyzed Layout and Text Object (ALTO)](https://loc.gov/standards/alto/techcenter/elementSet/index.html) was initially developed for the description of text OCR and layout information of pages for digitized material, and continues to be used by the Library of Congress. The goal was to describe the text's layout in a form to reconstruct the original content. Similarly, [hOCR](https://archive.org/developers/ocr.html) is another format for representing OCR output that is popular with Internet Archive, a non-profit digital library.

## Step 4: Tabular Extraction - Organizing the OCR Output
The last step in building out the OCR project is to take the image output from the above steps into a readable and accessible dataframe (i.e. csv). Tabular extraction is the process of separating a table from a large document, possibly recognizing individual rows, columns, or elements. Through this process, one now can view a text from a newspaper or magazine, in an organized spreadsheet.



